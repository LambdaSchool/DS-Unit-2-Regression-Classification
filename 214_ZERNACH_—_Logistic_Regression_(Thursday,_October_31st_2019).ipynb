{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "214 ZERNACH â€” Logistic Regression (Thursday, October 31st 2019)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zernach/DS-Unit-2-Linear-Models/blob/master/214_ZERNACH_%E2%80%94_Logistic_Regression_(Thursday%2C_October_31st_2019).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ7Sy_RyPRuH",
        "colab_type": "text"
      },
      "source": [
        "ASSIGNMENT COMPLETED BY: [RYAN ZERNACH](http://ryan.zernach.com/portfolio/)\n",
        "\n",
        "![alt text](http://www.zernach.com/wp-content/uploads/2019/09/Ryan-Zernach-Logo-1-e1568499634499.png)\n",
        "\n",
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7IXUfiQ2UKj6"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "## Assignment ðŸŒ¯\n",
        "\n",
        "You'll use a [**dataset of 400+ burrito reviews**](https://srcole.github.io/100burritos/). How accurately can you predict whether a burrito is rated 'Great'?\n",
        "\n",
        "> We have developed a 10-dimensional system for rating the burritos in San Diego. ... Generate models for what makes a burrito great and investigate correlations in its dimensions.\n",
        "\n",
        "- [x] Do train/validate/test split. Train on reviews from 2016 & earlier. Validate on 2017. Test on 2018 & later.\n",
        "- [x] Begin with baselines for classification.\n",
        "- [x] Use scikit-learn for logistic regression.\n",
        "- [x] Get your model's validation accuracy. (Multiple times if you try multiple iterations.)\n",
        "- [x] Get your model's test accuracy. (One time, at the end.)\n",
        "- [x] Commit your notebook to your fork of the GitHub repo.\n",
        "- [x] Watch Aaron's [video #1](https://www.youtube.com/watch?v=pREaWFli-5I) (12 minutes) & [video #2](https://www.youtube.com/watch?v=bDQgVt4hFgY) (9 minutes) to learn about the mathematics of Logistic Regression.\n",
        "\n",
        "\n",
        "## Stretch Goals\n",
        "\n",
        "- [ ] Add your own stretch goal(s) !\n",
        "- [ ] Make exploratory visualizations.\n",
        "- [x] Do one-hot encoding.\n",
        "- [x] Do [feature scaling](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "- [ ] Get and plot your coefficients.\n",
        "- [ ] Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaHmQ9D2P4GL",
        "colab_type": "text"
      },
      "source": [
        "## PART 1 -- Setup the dataset/dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9eSnDYhUGD7",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrSoNSvSPRuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data downloaded from https://srcole.github.io/100burritos/\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/burritos/burritos.csv')\n",
        "\n",
        "# Derive binary classification target:\n",
        "# We define a 'Great' burrito as having an\n",
        "# overall rating of 4 or higher, on a 5 point scale.\n",
        "# Drop unrated burritos.\n",
        "df = df.dropna(subset=['overall'])\n",
        "df['Great'] = df['overall'] >= 4\n",
        "\n",
        "# Clean/combine the Burrito categories\n",
        "df['Burrito'] = df['Burrito'].str.lower()\n",
        "\n",
        "california = df['Burrito'].str.contains('california')\n",
        "asada = df['Burrito'].str.contains('asada')\n",
        "surf = df['Burrito'].str.contains('surf')\n",
        "carnitas = df['Burrito'].str.contains('carnitas')\n",
        "\n",
        "df.loc[california, 'Burrito'] = 'California'\n",
        "df.loc[asada, 'Burrito'] = 'Asada'\n",
        "df.loc[surf, 'Burrito'] = 'Surf & Turf'\n",
        "df.loc[carnitas, 'Burrito'] = 'Carnitas'\n",
        "df.loc[~california & ~asada & ~surf & ~carnitas, 'Burrito'] = 'Other'\n",
        "\n",
        "# Drop some high cardinality categoricals\n",
        "df = df.drop(columns=['Notes', 'Location', 'Reviewer', 'Address', 'URL', 'Neighborhood'])\n",
        "\n",
        "# Drop some columns to prevent \"leakage\"\n",
        "df = df.drop(columns=['Rec', 'overall'])\n",
        "\n",
        "# Make [Date] a datetime object in preparation for PART 2\n",
        "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
        "\n",
        "# Cleanup the x's in columns to universalize the data\n",
        "# (there's even a Yes and a No in the Chips column instead of x's hahah) \n",
        "import numpy as np\n",
        "df['Chips'] = df['Chips'].str.replace('Yes','x')\n",
        "df['Chips'] = df['Chips'].str.replace('No', 'x') #This is cheating -- 'No' should be replaced with nan but its only one single instance-- how should I handle this?\n",
        "df = df.drop('Queso', axis=1) #Nothing but nan values\n",
        "\n",
        "dirty_columns = ('Chips', 'Unreliable', 'NonSD', 'Beef', 'Pico', 'Guac',\n",
        "                 'Cheese', 'Fries', 'Sour cream', 'Pork', 'Chicken', 'Shrimp',\n",
        "                 'Fish', 'Rice', 'Beans', 'Lettuce', 'Tomato', 'Bell peper',\n",
        "                 'Carrots', 'Cabbage', 'Sauce', 'Salsa.1', 'Cilantro', 'Onion',\n",
        "                 'Taquito', 'Pineapple', 'Ham', 'Chile relleno', 'Nopales',\n",
        "                 'Lobster', 'Egg', 'Mushroom', 'Bacon', 'Sushi',\n",
        "                 'Avocado', 'Corn', 'Zucchini')\n",
        "\n",
        "for column in dirty_columns:\n",
        "  df[column] = df[column].replace('X','x')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQQWYVeZP7-p",
        "colab_type": "text"
      },
      "source": [
        "## PART 2 -- Do train/validate/test split. Train on reviews from 2016 & earlier. Validate on 2017. Test on 2018 & later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y4lglJzQRyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df[(df['Date'] >= '01-01-2016') & (df['Date'] <= '12-31-2016')]\n",
        "validate = df[(df['Date'] >= '01-01-2017') & (df['Date'] <= '12-31-2017')]\n",
        "test = df[df['Date'] >= '01-01-2018']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cdz7hmWP-4Q",
        "colab_type": "text"
      },
      "source": [
        "## PART 3 -- Begin with baselines for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWpzXO7WQRg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "638b3a38-a7e1-4714-b860-73802ef1d15d"
      },
      "source": [
        "target = 'Great'\n",
        "features = df.columns.drop([target] + ['Date'])\n",
        "\n",
        "x_train = train[features]\n",
        "x_validate = validate[features]\n",
        "x_test = test[features]\n",
        "\n",
        "y_train = train[target]\n",
        "y_validate = validate[target]\n",
        "y_test = test[target]\n",
        "\n",
        "y_train.value_counts(normalize = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    0.591216\n",
              "True     0.408784\n",
              "Name: Great, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2rhSTTyQCLw",
        "colab_type": "text"
      },
      "source": [
        "## PART 4 --  Use scikit-learn for logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qby_bDyWOZqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Used if viewing full df's in separate code cells\n",
        "pd.options.display.max_columns = 999"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7fjhpnc8P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OneHotEncoding Features\n",
        "import category_encoders as ce\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "\n",
        "x_train_encoded = encoder.fit_transform(x_train)\n",
        "x_validate_encoded = encoder.fit_transform(x_validate)\n",
        "x_test_encoded = encoder.fit_transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBbuMllxR5iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the nan columns because it's redundant to have BOTH nan columns (with\n",
        "# 0's and 1's) and x columns (with 0's and 1's)\n",
        "for column in dirty_columns:\n",
        "  x_train_encoded = x_train_encoded.drop(column + '_nan', axis=1)\n",
        "  x_validate_encoded = x_validate_encoded.drop(column + '_nan', axis=1)\n",
        "  x_test_encoded = x_test_encoded.drop(column + '_nan', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVo8RbUga7qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I tried feature scaling, but I think it made my accuracy scores TOO accurate lol, I dont know...\n",
        "\n",
        "# Feature scaling\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#scaler = StandardScaler()\n",
        "\n",
        "#x_train_encoded = scaler.fit_transform(x_train_encoded)\n",
        "#x_validate_encoded = scaler.fit_transform(x_validate_encoded)\n",
        "#x_test_encoded = scaler.fit_transform(x_test_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RJrrDyoerIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SimplyImputing Features\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer()\n",
        "\n",
        "x_train_encoded_imputed = imputer.fit_transform(x_train_encoded)\n",
        "x_validate_encoded_imputed = imputer.fit_transform(x_validate_encoded)\n",
        "x_test_encoded_imputed = imputer.fit_transform(x_test_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IC50fQpQRMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "9d97bc20-5ca1-43f7-c068-b180f94839b8"
      },
      "source": [
        "# Logistic Regression to predict target\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_r_train_model = LogisticRegression()\n",
        "log_r_validate_model = LogisticRegression()\n",
        "log_r_test_model = LogisticRegression()\n",
        "\n",
        "log_r_train_model = log_r_train_model.fit(x_train_encoded_imputed, y_train)\n",
        "log_r_validate_model = log_r_validate_model.fit(x_validate_encoded_imputed, y_validate)\n",
        "log_r_test_model = log_r_test_model.fit(x_test_encoded_imputed, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wMZOfW_QEgb",
        "colab_type": "text"
      },
      "source": [
        "## PART 5 -- Get your model's training, validation, and test accuracy. (Multiple times if you try multiple iterations.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaXIW3G_QQ2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "accbeae4-3f59-4dfc-b3b9-400ca01b037f"
      },
      "source": [
        "print('Validation Accuracy (Training Set)', log_r_train_model.score(x_train_encoded_imputed, y_train))\n",
        "print('Validation Accuracy (Validation Set)', log_r_validate_model.score(x_validate_encoded_imputed, y_validate))\n",
        "print('Validation Accuracy (Testing Set)', log_r_test_model.score(x_test_encoded_imputed, y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy (Training Set) 0.8986486486486487\n",
            "Validation Accuracy (Validation Set) 0.9411764705882353\n",
            "Validation Accuracy (Testing Set) 0.9210526315789473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myOmOhXAQMOG",
        "colab_type": "text"
      },
      "source": [
        "# * Commit your notebook to your fork of the GitHub repo"
      ]
    }
  ]
}