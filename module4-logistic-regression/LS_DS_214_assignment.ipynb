{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_214_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xubxMZwnwYh"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgxtpJujnwYl"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_7y9x8OnwYm"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKdYs5PXoYrz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7bYhCERnwYn"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c11VsjKpnwYn"
      },
      "source": [
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date')\n",
        "    \n",
        "    # Format Column Named\n",
        "    df.columns = df.columns.str.upper().str.replace(' ', '_')\n",
        "\n",
        "    # Drop unrated burritos\n",
        "    df = df.dropna(subset=['OVERALL'])\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['GREAT'] = (df['OVERALL'] >= 4).astype('uint8')\n",
        "    \n",
        "    # Drop high cardinality categoricals\n",
        "    df = df.drop(columns=['NOTES', 'LOCATION', 'ADDRESS', 'URL', 'NEIGHBORHOOD'])\n",
        "    \n",
        "    # Drop columns to prevent \"leakage\"\n",
        "    df = df.drop(columns=['REC', 'OVERALL'])\n",
        "\n",
        "    #Binary Encoding\n",
        "    binary_encoding_needed = (set(df.nunique()[df.nunique().isin(range(0,6))].index) \n",
        "                              & set(df.dtypes[ df.dtypes=='object' ].index))\n",
        "    for col in binary_encoding_needed:\n",
        "      df.loc[ (df.loc[:][col].str.upper() == 'X') | \n",
        "             (df.loc[:][col].str.upper() == 'YES'), col] = 1\n",
        "      df.loc[ (df.loc[:][col].isnull()) |\n",
        "             (df.loc[:][col].astype('string').str.upper() == 'NO'), col] = 0\n",
        "      df[col] = df[col].astype('uint8')\n",
        "\n",
        "    #Burrito Formatting\n",
        "    n_most_freq_burrito_types = 4\n",
        "    df['BURRITO'] = ( 'BURRITO_' + df['BURRITO'].str.strip()\n",
        "                      .str.upper().str.replace(' ', '_') )\n",
        "    burrito_types = list(df['BURRITO'].value_counts().head(10).index)\n",
        "    df.loc[ ~(df['BURRITO'].isin(burrito_types)), 'BURRITO' ] = 'OTHER'\n",
        "    df = ( df.join(pd.get_dummies(df['BURRITO'])).\n",
        "           drop(columns=['OTHER', 'BURRITO']) )\n",
        "\n",
        "    #Drop Columns\n",
        "    threshhold = 10 #Min # Of Uniques For High Cardinality\n",
        "    drop = [col for col in df if ( df[col].nunique()<=1 \n",
        "            or (df[col].dtype=='object' and df[col].nunique()>threshhold) )]\n",
        "    df = df.drop(columns=drop)\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "\n",
        "df = wrangle(filepath)"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFGshNsgnwYo"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAL5FE8hnwYp"
      },
      "source": [
        "**Task 2:** Change the `wrangle` function so that the beef columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSYR4zYynwYq",
        "outputId": "4ebed87a-7347-4b02-9791-762bf494dc4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above.\n",
        "\n",
        "def show_col(df):\n",
        "  for i, col in enumerate(list(df.columns)):\n",
        "    output = ('('+str(i+1)+')'+col+': '\n",
        "              +str(df[col].nunique())+' unique '+str(df[col].dtype)+'s')\n",
        "    if (i+1)%3 != 0:\n",
        "      print(output, end='   ')\n",
        "    else:\n",
        "      print(output)\n",
        "\n",
        "show_col(df); print()\n",
        "\n",
        "print(\"Data Types:\", list(df.dtypes.unique()))"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1)YELP: 6 unique float64s   (2)GOOGLE: 18 unique float64s   (3)CHIPS: 2 unique uint8s\n",
            "(4)COST: 98 unique float64s   (5)HUNGER: 25 unique float64s   (6)MASS_(G): 18 unique float64s\n",
            "(7)DENSITY_(G/ML): 21 unique float64s   (8)LENGTH: 29 unique float64s   (9)CIRCUM: 30 unique float64s\n",
            "(10)VOLUME: 64 unique float64s   (11)TORTILLA: 18 unique float64s   (12)TEMP: 18 unique float64s\n",
            "(13)MEAT: 23 unique float64s   (14)FILLINGS: 22 unique float64s   (15)MEAT:FILLING: 25 unique float64s\n",
            "(16)UNIFORMITY: 28 unique float64s   (17)SALSA: 27 unique float64s   (18)SYNERGY: 27 unique float64s\n",
            "(19)WRAP: 23 unique float64s   (20)UNRELIABLE: 2 unique uint8s   (21)NONSD: 2 unique uint8s\n",
            "(22)BEEF: 2 unique uint8s   (23)PICO: 2 unique uint8s   (24)GUAC: 2 unique uint8s\n",
            "(25)CHEESE: 2 unique uint8s   (26)FRIES: 2 unique uint8s   (27)SOUR_CREAM: 2 unique uint8s\n",
            "(28)PORK: 2 unique uint8s   (29)CHICKEN: 2 unique uint8s   (30)SHRIMP: 2 unique uint8s\n",
            "(31)FISH: 2 unique uint8s   (32)RICE: 2 unique uint8s   (33)BEANS: 2 unique uint8s\n",
            "(34)LETTUCE: 2 unique uint8s   (35)TOMATO: 2 unique uint8s   (36)BELL_PEPER: 2 unique uint8s\n",
            "(37)CARROTS: 2 unique uint8s   (38)CABBAGE: 2 unique uint8s   (39)SAUCE: 2 unique uint8s\n",
            "(40)SALSA.1: 2 unique uint8s   (41)CILANTRO: 2 unique uint8s   (42)ONION: 2 unique uint8s\n",
            "(43)TAQUITO: 2 unique uint8s   (44)PINEAPPLE: 2 unique uint8s   (45)HAM: 2 unique uint8s\n",
            "(46)CHILE_RELLENO: 2 unique uint8s   (47)NOPALES: 2 unique uint8s   (48)LOBSTER: 2 unique uint8s\n",
            "(49)EGG: 2 unique uint8s   (50)MUSHROOM: 2 unique uint8s   (51)BACON: 2 unique uint8s\n",
            "(52)SUSHI: 2 unique uint8s   (53)AVOCADO: 2 unique uint8s   (54)CORN: 2 unique uint8s\n",
            "(55)ZUCCHINI: 2 unique uint8s   (56)GREAT: 2 unique uint8s   (57)BURRITO_ADOBADA: 2 unique uint8s\n",
            "(58)BURRITO_AL_PASTOR: 2 unique uint8s   (59)BURRITO_CALIFORNIA: 2 unique uint8s   (60)BURRITO_CALIFORNIA_EVERYTHING: 2 unique uint8s\n",
            "(61)BURRITO_CARNE_ASADA: 2 unique uint8s   (62)BURRITO_CARNITAS: 2 unique uint8s   (63)BURRITO_HOLY_MOLY: 2 unique uint8s\n",
            "(64)BURRITO_LOCAL: 2 unique uint8s   (65)BURRITO_SURFIN_CALIFORNIA: 2 unique uint8s   (66)BURRITO_SURF_&_TURF: 2 unique uint8s\n",
            "\n",
            "Data Types: [dtype('float64'), dtype('uint8')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s1Di1GSnwYr"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MamLFDvbnwYs"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGWEmADcnwYs"
      },
      "source": [
        "target = 'GREAT'\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqoYZr17nwYs"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USvPSMoJOBKa",
        "outputId": "ceeb7747-8392-4fb6-f1ab-7b925e165255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.index"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2011-05-16', '2015-04-20', '2016-01-18', '2016-01-24',\n",
              "               '2016-01-24', '2016-01-24', '2016-01-24', '2016-01-24',\n",
              "               '2016-01-24', '2016-01-24',\n",
              "               ...\n",
              "               '2019-08-27', '2019-08-27', '2019-08-27', '2019-08-27',\n",
              "               '2019-08-27', '2019-08-27', '2019-08-27', '2019-08-27',\n",
              "               '2019-08-27', '2026-04-25'],\n",
              "              dtype='datetime64[ns]', name='Date', length=2119, freq=None)"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quZRK8UgnwYt"
      },
      "source": [
        "mask = (df.index.year == 2016) | (df.index.year == 2017)\n",
        "X_train, y_train = X[mask], y[mask]\n",
        "X_test, y_test = X[~mask], y[~mask]"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48m193evnwYt"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKjSVPhNOluU",
        "outputId": "8fd3c425-8128-4eec-f3a0-94e38b94f747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "pd.DataFrame(y_train.value_counts(normalize=True)*100)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GREAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>61.972547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38.027453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       GREAT\n",
              "0  61.972547\n",
              "1  38.027453"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygAmKShhnwYt",
        "outputId": "27758154-9539-4a8f-833d-3e4e556632a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "baseline_acc = y_train.value_counts(normalize=True).max()\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy Score: 0.619725470259278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aLS-P75nwYu"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBYlSt2TnwYu",
        "outputId": "1cdb7540-e49b-452b-8fe6-25e354373c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_logr = make_pipeline(\n",
        "    OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()\n",
        ")\n",
        "model_logr.fit(X_train, y_train)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('onehotencoder', OneHotEncoder(cols=[], use_cat_names=True)),\n",
              "                ('simpleimputer', SimpleImputer()),\n",
              "                ('standardscaler', StandardScaler()),\n",
              "                ('logisticregression', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1520lanwYu"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtX7kVZLnwYu"
      },
      "source": [
        "training_acc = ...\n",
        "test_acc = ...\n",
        "\n",
        "print('Training Accuracy:', training_acc)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huKrMo3OnwYv"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mk7Qi2ZnwYv"
      },
      "source": [
        "# Create your horizontal barchart here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LKKv0cSnwYv"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLFjTCqlnwYv"
      },
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQfOLd8onwYw"
      },
      "source": [
        "**Give your written answer here:**\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```"
      ]
    }
  ]
}